org: janelia
service: burst-compute


plugins:
  - serverless-bundle
  - serverless-step-functions


custom:
  org: ${opt:org, self:org}
  version: ${file(./package.json):version}
  tracing: false
  debug: true
  dispatchFunctionName: ${self:custom.org}-${self:service}-${self:provider.stage}-dispatch
  dispatchNextBatchesFunctionName: ${self:custom.org}-${self:service}-${self:provider.stage}-dispatchNextBatches
  monitorFunctionName: ${self:custom.org}-${self:service}-${self:provider.stage}-monitor
  cleanupFunctionName: ${self:custom.org}-${self:service}-${self:provider.stage}-cleanupBatch
  tasksTable: ${self:custom.org}-${self:service}-${self:provider.stage}-tasks
  mapJobsInputsBucketName: ${self:custom.org}-${self:service}-${self:provider.stage}-mapjobsinputs
  stateMachine: ${self:custom.org}-${self:service}-${self:provider.stage}-lifecycle
  defaultJobTimeoutSecs: 600
  maxConcurrentJobs: ${env:MAX_PARALLELISM, 5000}
  maxBranchingFactor: ${env:MAX_BRANCHING_FACTOR, 2}


provider:
  name: aws
  architecture: arm64
  runtime: nodejs20.x
  region: ${opt:region, "us-east-1"}
  stage: ${opt:stage, "dev"}
  tags:
    PROJECT: BurstCompute
    VERSION: ${self:custom.version}
    DEVELOPER: ${env:USER}
  stackTags:
    PROJECT: BurstCompute
    VERSION: ${self:custom.version}
    DEVELOPER: ${env:USER}
    STAGE: ${self:provider.stage}
  tracing:
    apiGateway: false
    lambda: ${self:custom.tracing}
  environment:
    AWS_NODEJS_CONNECTION_REUSE_ENABLED: 1
  iam:
    role:
      statements:
      - Effect: Allow
        Action:
          - dynamodb:Query
          - dynamodb:PutItem
        Resource: "arn:aws:dynamodb:${self:provider.region}:*:table/${self:custom.tasksTable}"
      - Effect: Allow
        Action:
          - states:StartExecution
        Resource: "arn:aws:states:${self:provider.region}:*:stateMachine:${self:custom.stateMachine}"
      - Effect: Allow
        Action:
          - lambda:InvokeFunction
        Resource: "*"
      - Effect: Allow
        Action:
          - s3:GetObject
          - s3:ListBucket
          - s3:PutObject
          - s3:DeleteObject
        Resource:
          - "arn:aws:s3:::${self:custom.mapJobsInputsBucketName}"
          - "arn:aws:s3:::${self:custom.mapJobsInputsBucketName}/*"


package:
  individually: true
  patterns:
    - src/main/nodejs/**


functions:

  dispatch:
    name: ${self:custom.dispatchFunctionName}
    handler: src/main/nodejs/dispatch.dispatchHandler
    memorySize: 128
    # 5 minute timeout
    timeout: 300
    environment:
      DEBUG: ${self:custom.debug}
      JOB_TIMEOUT_SECS: ${self:custom.defaultJobTimeoutSecs}
      DEFAULT_CONCURRENT_JOBS: ${self:custom.maxConcurrentJobs}
      MAX_BRANCHING_FACTOR: ${self:custom.maxBranchingFactor}
      DISPATCH_FUNCTION_NAME: ${self:custom.dispatchNextBatchesFunctionName}
      MONITOR_FUNCTION_NAME: ${self:custom.monitorFunctionName}
      STATE_MACHINE_ARN: ${self:resources.Outputs.JobLifecycleStateMachine.Value}
      MAP_JOBS_INPUTS_BUCKET_NAME: ${self:custom.mapJobsInputsBucketName}
      TASKS_TABLE_NAME: ${self:custom.tasksTable}

  dispatchNextBatches:
    name: ${self:custom.dispatchNextBatchesFunctionName}
    handler: src/main/nodejs/dispatch.dispatchNextBatchesHandler
    memorySize: 128
    # 5 minute timeout
    timeout: 300
    environment:
      DEBUG: ${self:custom.debug}

  cleanupBatch:
    name: ${self:custom.cleanupFunctionName}
    handler: src/main/nodejs/cleanup.cleanupBatchHandler
    memorySize: 128
    # 5 minute timeout
    timeout: 300
    environment:
      DEBUG: ${self:custom.debug}

  monitor:
    name: ${self:custom.monitorFunctionName}
    handler: src/main/nodejs/monitor.monitorHandler
    memorySize: 128
    timeout: ${self:custom.defaultJobTimeoutSecs}
    environment:
      DEBUG: ${self:custom.debug}


stepFunctions:
  validate: true # enable pre-deployment definition validation
  stateMachines:

    jobLifecycleStateMachine:
      id: JobLifecycleStateMachine
      name: ${self:custom.stateMachine}
      definition:
        Comment: "Monitors a burst compute job and calls the combiner when all tasks are done"
        StartAt: MapJobs
        States:
          MapJobs:
            Type: Map
            InputPath: "$"
            ItemReader:
              Resource: "arn:aws:states:::s3:getObject"
              ReaderConfig:
                InputType: JSON
              Parameters:
                Bucket.$: "$$.Execution.Input.mapJobsInputBucket"
                Key.$: "$.inputJobsFile"
            ItemSelector:
              jobId.$: "$$.Execution.Input.jobId"
              startTime.$: "$$.Execution.Input.startTime"
              numBatches.$: "$$.Execution.Input.numBatches"
              jobsTimeoutSecs.$: "$$.Execution.Input.jobsTimeoutSecs"
              jobParameters.$: "$$.Execution.Input.jobParameters"
              workerFunctionName.$: "$$.Execution.Input.workerFunctionName"
              tasksTableName.$: "$$.Execution.Input.tasksTableName"
              batchId.$: "$$.Map.Item.Value.batchId"
              startIndex.$: "$$.Map.Item.Value.startIndex"
              endIndex.$: "$$.Map.Item.Value.endIndex"
            ToleratedFailurePercentagePath: "$.toleratedPercentageFailure"
            ItemProcessor:
              ProcessorConfig:
                Mode: DISTRIBUTED
                ExecutionType: STANDARD
              StartAt: ExecuteBatch
              States:
                ExecuteBatch:
                  Type: Task
                  Resource: arn:aws:states:::lambda:invoke
                  Parameters:
                    FunctionName.$: "$$.Execution.Input.workerFunctionName"
                    InvocationType: "RequestResponse"
                    Payload.$: "$"
                  OutputPath: "$.Payload"
                  Next: PersistResults
                PersistResults:
                  Type: Task
                  Resource: arn:aws:states:::dynamodb:putItem
                  Parameters:
                    TableName.$: "$$.Execution.Input.tasksTableName"
                    Item:
                      jobId.$: "$.jobId"
                      batchId: 
                        N.$: "$.batchId"
                      resultsMimeType.$: "$.resultsMimeType"
                      results.$: "$.results"
                      nresults:
                        N.$: "$.nresults"
                      ttl:
                        N.$: "$.ttl"
                  ResultPath: null
                  Retry:
                    - ErrorEquals:
                      - DynamoDB.AmazonDynamoDBException
                      - States.ThrottlingException
                      IntervalSeconds: 2
                      MaxAttempts: 3
                      BackoffRate: 2.0
                  End: true
            ResultPath: null
            Catch:
              - ErrorEquals:
                - States.ALL
                Next: CleanupBatchInput
                ResultPath: "$.batchErrors[0]"
            Next: CleanupBatchInput
          CleanupBatchInput:
            Type: Task
            Comment: Cleanup batch input file
            Resource: arn:aws:states:::lambda:invoke
            Parameters:
              FunctionName: "${self:custom.cleanupFunctionName}"
              InvocationType: "RequestResponse"
              Payload.$: "$"
            OutputPath: "$.Payload"
            Catch:
              - ErrorEquals:
                - States.ALL
                Next: AreWeDone # ignore errors
                ResultPath: null
            Next: AreWeDone
          AreWeDone:
            Type: Choice
            Comment: Check if the computation completed
            Choices:
              - Variable: "$.lastBatchId"
                NumericLessThanPath: "$.numBatches"
                Next: Monitor
            Default: MarkAsCompleted
          MarkAsCompleted:
            Type: Pass
            Comment: Inject completed attribute
            Result:
              completed: true
            Next: Combine
          Monitor:
            Type: Task
            Resource: arn:aws:states:::lambda:invoke
            Parameters:
              FunctionName.$: "$$.Execution.Input.monitorFunctionName"
              Payload:
                jobId.$: "$$.Execution.Input.jobId"
                startTime.$: "$$.Execution.Input.startTime"
                firstBatchId.$: "$.firstBatchId"
                lastBatchId.$: "$.lastBatchId"
                numBatches.$: "$$.Execution.Input.numBatches"
                jobsTimeoutSecs.$: "$$.Execution.Input.jobsTimeoutSecs"
                tasksTableName.$: "$$.Execution.Input.tasksTableName"
                toleratedPercentageFailure.$: "$$.Execution.Input.toleratedPercentageFailure"
            OutputPath: "$.Payload"
            Retry:
              - ErrorEquals: 
                - Lambda.TooManyRequestsException
                IntervalSeconds: 1
                MaxAttempts: 3
            Catch:
              - ErrorEquals:
                - States.ALL
                Next: IsTimedOut
                ResultPath: "$.batchErrors[0]"
            Next: IsTimedOut
          IsTimedOut:
            Type: Choice
            Choices:
              - Variable: "$.timedOut"
                BooleanEquals: true
                Next: CombineError
            Default: DispatchNextJobs
          DispatchNextJobs:
            Type: Task
            Resource: arn:aws:states:::lambda:invoke
            Parameters:
              FunctionName.$: "$$.Execution.Input.dispatchFunctionName"
              InvocationType: "RequestResponse"
              Payload:
                jobId.$: "$$.Execution.Input.jobId"
                datasetStartIndex.$: "$$.Execution.Input.datasetStartIndex"
                datasetEndIndex.$: "$$.Execution.Input.datasetEndIndex"
                batchSize.$: "$$.Execution.Input.batchSize"
                firstBatchId.$: "$.firstBatchId"
                lastBatchId.$: "$.lastBatchId"
                maxParallelism.$: "$$.Execution.Input.maxParallelism"
                numBatches.$: "$$.Execution.Input.numBatches"
                jobParameters.$: "$$.Execution.Input.jobParameters"
                mapJobsInputBucket.$: "$$.Execution.Input.mapJobsInputBucket"
                toleratedPercentageFailure.$: "$$.Execution.Input.toleratedPercentageFailure"
            OutputPath: "$.Payload"
            Next: MapJobs
          Combine:
            Type: Task
            Resource: arn:aws:states:::lambda:invoke
            Parameters:
              FunctionName.$: "$$.Execution.Input.combinerFunctionName"
              Payload:
                jobId.$: "$$.Execution.Input.jobId"
                datasetStartIndex.$: "$$.Execution.Input.datasetStartIndex"
                datasetEndIndex.$: "$$.Execution.Input.datasetEndIndex"
                batchSize.$: "$$.Execution.Input.batchSize"
                startTime.$: "$$.Execution.Input.startTime"
                numBatches.$: "$$.Execution.Input.numBatches"
                completed.$: "$.completed"
                jobsTimeoutSecs.$: "$$.Execution.Input.jobsTimeoutSecs"
                jobParameters.$: "$$.Execution.Input.jobParameters"
                tasksTableName.$: "$$.Execution.Input.tasksTableName"
            OutputPath: "$"
            Catch:
              - ErrorEquals:
                - States.ALL
                Next: CombineError
                ResultPath: "$.batchErrors[0]"
            Next: EndState
          CombineError:
            Type: Task
            Resource: arn:aws:states:::lambda:invoke
            Parameters:
              FunctionName.$: "$$.Execution.Input.combinerFunctionName"
              Payload:
                jobId.$: "$$.Execution.Input.jobId"
                datasetStartIndex.$: "$$.Execution.Input.datasetStartIndex"
                datasetEndIndex.$: "$$.Execution.Input.datasetEndIndex"
                batchSize.$: "$$.Execution.Input.batchSize"
                startTime.$: "$$.Execution.Input.startTime"
                numBatches.$: "$$.Execution.Input.numBatches"
                jobsTimeoutSecs.$: "$$.Execution.Input.jobsTimeoutSecs"
                jobParameters.$: "$$.Execution.Input.jobParameters"
                tasksTableName.$: "$$.Execution.Input.tasksTableName"
                batchErrors.$: "$.batchErrors"
            Next: EndState
          EndState:
            Type: Pass
            End: true


resources:

  Resources:
    TasksTable:
      Type: AWS::DynamoDB::Table
      Properties:
        TableName: ${self:custom.tasksTable}
        KeySchema:
          - AttributeName: jobId
            KeyType: HASH
          - AttributeName: batchId
            KeyType: RANGE
        AttributeDefinitions:
          - AttributeName: jobId
            AttributeType: S
          - AttributeName: batchId
            AttributeType: N
        BillingMode: PAY_PER_REQUEST
        TimeToLiveSpecification:
          AttributeName: ttl
          Enabled: true

    MapJobsInputsBucket:
      Type: AWS::S3::Bucket
      DeletionPolicy: Delete
      Properties:
        BucketName: ${self:custom.mapJobsInputsBucketName}
        AccessControl: Private
          

  Outputs:
    TasksTable:
      Description: Name of the tasks table
      Value:
        Ref: TasksTable
      Export:
        Name: BurstComputeTasksTable-${self:provider.stage}
    MapJobsInputsBucket:
      Description: Name of the bucket containing inputs for distributed map jobs
      Value:
        Ref: MapJobsInputsBucket
    JobLifecycleStateMachine:
      Description: The ARN of the state machine
      Value:
        Ref: JobLifecycleStateMachine
    DispatchLambdaFunction:
      Description: Name of the dispatch function
      Value:
        Ref: DispatchLambdaFunction
      Export:
        Name: BurstComputeDispatchLambdaFunction-${self:provider.stage}
    MonitorLambdaFunction:
      Description: Name of the monitor function
      Value:
        Ref: MonitorLambdaFunction
      Export:
        Name: BurstComputeMonitorLambdaFunction-${self:provider.stage}
